{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1kWMDnBmSQSV","colab_type":"text"},"source":["\n","\n","**Loading Saved Model**"]},{"cell_type":"code","metadata":{"id":"8M9q-rBO13Uo","colab_type":"code","colab":{}},"source":["from keras.models import load_model\n","from keras.models import Sequential\n","from keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","import numpy as np\n","from keras.optimizers import SGD\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os\n","from random import shuffle\n","#model = load_model('model-accuracy-99%.h5')\n","test_dir = r'/content/action/'\n","test1_dir = r'E:\\Nikhil\\python\\videoclass\\videoclass\\test\\test1'\n","test2_dir=r'E:\\Nikhil\\python\\machinelearningex\\videoclass\\videoclass\\test'\n","train1_data=r'/content/action/train'\n","validation1_data=r'/content/action/test'\n","model=Sequential()\n","model=load_model('model4-imggen.h5')\n","lrate=0.01\n","epochs1=50\n","decay=lrate/epochs1\n","sgd = SGD(lr=lrate,momentum=0.9,decay=decay,nesterov=False)\n","\n","model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n","test_datagen= ImageDataGenerator(rescale=1./255)\n","test_generator = test_datagen.flow_from_directory(\n","    test_dir,\n","    target_size=(120,120),\n","    classes=['testing'],\n","    shuffle=False,\n","    batch_size=10000,\n","    )\n","train_datagen = ImageDataGenerator(\n","    rescale=1. / 255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True)\n","train_generator = train_datagen.flow_from_directory(\n","    train1_data,\n","    target_size=(120,120),\n","    batch_size=32,\n","    shuffle=False,\n","    classes=['boxing1','handclapping','handwaving','jogging'],\n","    )\n","test_generator.reset()\n","train_generator.reset()\n","#print(test_generator.filenames)\n","#model1=model.predict_generator(test_generator,1200)\n","test_datagen = ImageDataGenerator(rescale=1. / 255)\n","validation_generator = test_datagen.flow_from_directory(\n","    validation1_data,\n","    target_size=(120,120),\n","    classes=['boxing','handclapping','handwaving','jogging'],\n","    batch_size=12,\n","    )\n","\n","\n","\n","\n","pred= model.predict_generator(test_generator, 1)\n","predicted_class_indices=np.argmax(pred,axis=1)\n","labels = (validation_generator.class_indices)\n","labels2 = dict((v,k) for k,v in labels.items())\n","predictions = [labels2[k] for k in predicted_class_indices]\n","#print(predicted_class_indices)\n","#print (labels)\n","            \n","#print (predictions)\n","\n","dt={}\n","for i in range(len(test_generator.filenames)):\n","        \n","    name=test_generator.filenames[i].split('\\\\')[-1]\n","    dt[name]=predictions[i]\n","    \n","\n","print(dt)\n","    \n","\n","\n","    \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gEFAHsFzDoh","colab_type":"code","outputId":"83b0e867-aa7f-4235-cb64-8fcdf23ddb5e","executionInfo":{"status":"ok","timestamp":1563801055542,"user_tz":-330,"elapsed":76108,"user":{"displayName":"NIKIL HC","photoUrl":"https://lh6.googleusercontent.com/-E4HGkZeIBNM/AAAAAAAAAAI/AAAAAAAAADU/qoO8DdBTQ7g/s64/photo.jpg","userId":"01370648069011997865"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["!git clone https://github.com/Nikhilhc/action.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'action'...\n","remote: Enumerating objects: 119383, done.\u001b[K\n","remote: Counting objects: 100% (119383/119383), done.\u001b[K\n","remote: Compressing objects: 100% (119379/119379), done.\u001b[K\n","remote: Total 119383 (delta 1), reused 119383 (delta 1), pack-reused 0\u001b[K\n","Receiving objects: 100% (119383/119383), 858.12 MiB | 15.45 MiB/s, done.\n","Resolving deltas: 100% (1/1), done.\n","Checking out files: 100% (126698/126698), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t8kO-PXQS_OJ","colab_type":"text"},"source":["**Packages Required For Training Model**"]},{"cell_type":"code","metadata":{"id":"zA-If4OLS7u4","colab_type":"code","colab":{}},"source":["\n","import keras\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import cv2\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import tflearn\n","from keras.preprocessing.image import ImageDataGenerator\n","from tflearn.layers.conv import conv_2d,max_pool_2d\n","from tflearn.layers.core import input_data,dropout,fully_connected\n","from tflearn.layers.estimator import regression\n","from random import shuffle\n","from keras.layers.convolutional import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.models import Sequential\n","from keras.constraints import maxnorm\n","from keras.optimizers import SGD\n","from keras.utils import np_utils\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import load_model\n","from keras.layers import GlobalMaxPooling2D"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CJIOTDgXTUte","colab_type":"text"},"source":["**Creating Training set and Testing dataset using tqdm**"]},{"cell_type":"code","metadata":{"id":"pDmD5lzqTNz0","colab_type":"code","colab":{}},"source":["\n","train1_data = '/content/action/action2/humanactivity/train'\n","validation1_data='/content/action/action2/humanactivity/val'\n","IMG_SIZE=120\n","IMG_SIZE1=120\n","LR = 1e-3\n","\n","def create_label(image_name):\n","    word_label=image_name.split('_')[-2]\n","    if word_label=='boxing':\n","        return 'boxing'\n","    if word_label =='handclapping':\n","        return 'handclapping'\n","    if word_label =='handwaving':\n","        return 'handwaving'\n","    if word_label == 'jogging':\n","        return 'jogging'\n","    if word_label == 'running':\n","        return 'running'\n","    if word_label == 'walking':\n","        return 'walking'\n","    \n","\n","def create_train_data():\n","    training_data = []\n","    for img in tqdm(os.listdir(train1_data)):\n","        path = os.path.join(train1_data,img)\n","        img_data = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n","        img_data=cv2.resize(img_data,(IMG_SIZE,IMG_SIZE1))\n","        training_data.append([np.array(img_data),create_label(img)])\n","    shuffle(training_data)\n","    np.save('train_data.npy',training_data)\n","    return training_data\n","\n","def create_test_data():\n","    test_data = []\n","    for img in tqdm(os.listdir(validation1_data)):\n","        path = os.path.join(validation1_data,img)\n","        img_data = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n","        img_data=cv2.resize(img_data,(IMG_SIZE,IMG_SIZE1))\n","        test_data.append([np.array(img_data),create_label(img)])\n","    shuffle(test_data)\n","    np.save('test_data.npy',test_data)\n","    return test_data\n","\n","train_data = create_train_data()\n","test_data=create_test_data()\n","\n","train = train_data[:-1]\n","test = train_data[-1200:]\n","\n","X_train = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE1,1)\n","y_train = [i[1] for i in train]\n","X_test = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE1,1)\n","y_test =[i[1] for i in test]\n","\n","X_train=X_train.astype('float32')\n","X_test=X_test.astype('float32')\n","print(y_test)\n","\n","encoder=LabelEncoder()\n","encoder.fit(y_test)\n","y_test=encoder.transform(y_test)\n","encoder.fit(y_train)\n","y_train=encoder.transform(y_train)\n","print(y_test)\n","\n","X_train=X_train/255.0\n","X_test=X_test/255.0\n","y_train=np_utils.to_categorical(y_train)\n","y_test=np_utils.to_categorical(y_test)\n","num_classes=6\n","print(y_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5k2Ik6bISyMI","colab_type":"text"},"source":["**Training The Model**"]},{"cell_type":"code","metadata":{"id":"dUtWQ12HpTk-","colab_type":"code","outputId":"4205e425-7e44-4478-e9bd-840544f90054","executionInfo":{"status":"error","timestamp":1563844893440,"user_tz":-330,"elapsed":59479,"user":{"displayName":"NIKIL HC","photoUrl":"https://lh6.googleusercontent.com/-E4HGkZeIBNM/AAAAAAAAAAI/AAAAAAAAADU/qoO8DdBTQ7g/s64/photo.jpg","userId":"01370648069011997865"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","import keras\n","import numpy as np\n","#from parse import load_data\n","from tqdm import tqdm\n","import os\n","import cv2\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import tflearn\n","from keras.preprocessing.image import ImageDataGenerator\n","from tflearn.layers.conv import conv_2d,max_pool_2d\n","from tflearn.layers.core import input_data,dropout,fully_connected\n","from tflearn.layers.estimator import regression\n","from random import shuffle\n","from keras.layers.convolutional import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.models import Sequential\n","from keras.constraints import maxnorm\n","from keras.optimizers import SGD\n","from keras.utils import np_utils\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import load_model\n","from keras.layers import GlobalMaxPooling2D\n","\n","\n","train1_data = '/content/action/action2/humanactivity/train'\n","validation1_data='/content/action/action2/humanactivity/val'\n","IMG_SIZE=120\n","IMG_SIZE1=120\n","LR = 1e-3\n","\n","model=Sequential()\n","model.add(Conv2D(32,3,3,input_shape=(IMG_SIZE,IMG_SIZE1,3),border_mode='same',\n","                        activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Conv2D(32,3,3,activation='relu',border_mode='same'\n","                        ))\n","model.add(MaxPooling2D(pool_size=(2,2),dim_ordering=\"th\"))\n","\n","model.add(Conv2D(64,3,3,activation='relu',border_mode='same'\n","                        ))\n","model.add(Dropout(0.2))\n","model.add(Conv2D(64,3,3,activation='relu',border_mode='same'))\n","model.add(MaxPooling2D(pool_size=(2,2),dim_ordering=\"th\"))\n","\n","\n","model.add(Conv2D(128,3,3,activation='relu',border_mode='same'))\n","model.add(Dropout(0.2))\n","model.add(Conv2D(128,3,3,activation='relu',border_mode='same'))\n","model.add(MaxPooling2D(pool_size=(2,2),dim_ordering=\"th\"))\n","\n","\n","\n","model.add(Flatten())\n","\n","\n","model.add(Dense(4,activation='softmax'))\n","\n","filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n","epochs1=50\n","lrate=0.01\n","decay=lrate/epochs1\n","sgd = SGD(lr=lrate,momentum=0.9,decay=decay,nesterov=False)\n","model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n","checkpoint=keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)\n","callbacks_lst=[checkpoint]\n","print(model.summary())\n","#model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=epochs1,batch_size=16,verbose=2)\n","#scores = model.evaluate(X_test,y_test,verbose=0)\n","#print(\"Accuracy:%.2f%%\"%(scores[1]*100))\n","shift=0.2\n","train_datagen = ImageDataGenerator(\n","    rescale=1. / 255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True\n","    )\n","test_datagen = ImageDataGenerator(rescale=1. / 255)\n","train_generator = train_datagen.flow_from_directory(\n","    train1_data,\n","    target_size=(IMG_SIZE, IMG_SIZE1),\n","    batch_size=128,\n","    classes=['boxing','handclapping','handwaving','walking'],\n","    )\n","\n","validation_generator = test_datagen.flow_from_directory(\n","    validation1_data,\n","    target_size=(IMG_SIZE,IMG_SIZE1),\n","    classes=['boxing','handclapping','handwaving','walking'],\n","    batch_size=128,\n","    )\n","\n","model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=130000// 128, #nb_train_samples//batch_size\n","    epochs=25,\n","    validation_data=validation_generator,\n","    callbacks=callbacks_lst,\n","    validation_steps=2000/128)\n","    \n","model.save('model5-imggen.h5')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","WARNING: Logging before flag parsing goes to stderr.\n","W0723 01:22:49.113529 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n","\n","W0723 01:22:49.115188 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","W0723 01:22:49.125541 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","W0723 01:22:49.132407 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","W0723 01:22:49.141719 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n","\n","W0723 01:22:49.143069 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","W0723 01:22:49.151765 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(120, 120,..., activation=\"relu\", padding=\"same\")`\n","W0723 01:22:49.179668 140093030946688 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:107: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:109: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n","W0723 01:22:49.858878 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:111: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:114: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:115: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:118: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:120: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:121: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n","W0723 01:22:50.140116 140093030946688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 120, 120, 32)      896       \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 120, 120, 32)      0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 120, 120, 32)      9248      \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 120, 60, 16)       0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 120, 60, 64)       9280      \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 120, 60, 64)       0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 120, 60, 64)       36928     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 120, 30, 32)       0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 120, 30, 128)      36992     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 120, 30, 128)      0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 120, 30, 128)      147584    \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 120, 15, 64)       0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 115200)            0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 4)                 460804    \n","=================================================================\n","Total params: 701,732\n","Trainable params: 701,732\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Found 100000 images belonging to 4 classes.\n"],"name":"stdout"},{"output_type":"stream","text":["W0723 01:22:52.898376 140093030946688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Found 2000 images belonging to 4 classes.\n","Epoch 1/25\n","1015/1015 [==============================] - 762s 750ms/step - loss: 0.9502 - acc: 0.5753 - val_loss: 0.4755 - val_acc: 0.8205\n","\n","Epoch 00001: val_acc improved from -inf to 0.82050, saving model to weights-improvement-01-0.82.hdf5\n","Epoch 2/25\n","1015/1015 [==============================] - 752s 740ms/step - loss: 0.4038 - acc: 0.8405 - val_loss: 0.2305 - val_acc: 0.9160\n","\n","Epoch 00002: val_acc improved from 0.82050 to 0.91600, saving model to weights-improvement-02-0.92.hdf5\n","Epoch 3/25\n","1015/1015 [==============================] - 749s 738ms/step - loss: 0.2645 - acc: 0.8977 - val_loss: 0.1720 - val_acc: 0.9395\n","\n","Epoch 00003: val_acc improved from 0.91600 to 0.93950, saving model to weights-improvement-03-0.94.hdf5\n","Epoch 4/25\n","1015/1015 [==============================] - 746s 735ms/step - loss: 0.1928 - acc: 0.9278 - val_loss: 0.1045 - val_acc: 0.9630\n","\n","Epoch 00004: val_acc improved from 0.93950 to 0.96300, saving model to weights-improvement-04-0.96.hdf5\n","Epoch 5/25\n","1015/1015 [==============================] - 745s 734ms/step - loss: 0.1474 - acc: 0.9458 - val_loss: 0.0983 - val_acc: 0.9635\n","\n","Epoch 00005: val_acc improved from 0.96300 to 0.96350, saving model to weights-improvement-05-0.96.hdf5\n","Epoch 6/25\n","1015/1015 [==============================] - 747s 736ms/step - loss: 0.1233 - acc: 0.9548 - val_loss: 0.1050 - val_acc: 0.9590\n","\n","Epoch 00006: val_acc did not improve from 0.96350\n","Epoch 7/25\n","1015/1015 [==============================] - 748s 737ms/step - loss: 0.1030 - acc: 0.9627 - val_loss: 0.0593 - val_acc: 0.9815\n","\n","Epoch 00007: val_acc improved from 0.96350 to 0.98150, saving model to weights-improvement-07-0.98.hdf5\n","Epoch 8/25\n","1015/1015 [==============================] - 747s 736ms/step - loss: 0.0889 - acc: 0.9677 - val_loss: 0.0549 - val_acc: 0.9855\n","\n","Epoch 00008: val_acc improved from 0.98150 to 0.98550, saving model to weights-improvement-08-0.99.hdf5\n","Epoch 9/25\n","1015/1015 [==============================] - 746s 735ms/step - loss: 0.0798 - acc: 0.9720 - val_loss: 0.0470 - val_acc: 0.9850\n","\n","Epoch 00009: val_acc did not improve from 0.98550\n","Epoch 10/25\n","1015/1015 [==============================] - 741s 730ms/step - loss: 0.0702 - acc: 0.9753 - val_loss: 0.0490 - val_acc: 0.9800\n","\n","Epoch 00010: val_acc did not improve from 0.98550\n","Epoch 11/25\n","1015/1015 [==============================] - 744s 733ms/step - loss: 0.0633 - acc: 0.9773 - val_loss: 0.0361 - val_acc: 0.9870\n","\n","Epoch 00011: val_acc improved from 0.98550 to 0.98700, saving model to weights-improvement-11-0.99.hdf5\n","Epoch 12/25\n","1015/1015 [==============================] - 742s 731ms/step - loss: 0.0580 - acc: 0.9795 - val_loss: 0.0317 - val_acc: 0.9890\n","\n","Epoch 00012: val_acc improved from 0.98700 to 0.98900, saving model to weights-improvement-12-0.99.hdf5\n","Epoch 13/25\n","1015/1015 [==============================] - 746s 735ms/step - loss: 0.0549 - acc: 0.9805 - val_loss: 0.0325 - val_acc: 0.9880\n","\n","Epoch 00013: val_acc did not improve from 0.98900\n","Epoch 14/25\n","1015/1015 [==============================] - 745s 734ms/step - loss: 0.0489 - acc: 0.9826 - val_loss: 0.0397 - val_acc: 0.9890\n","\n","Epoch 00014: val_acc did not improve from 0.98900\n","Epoch 15/25\n","1015/1015 [==============================] - 745s 734ms/step - loss: 0.0444 - acc: 0.9846 - val_loss: 0.0352 - val_acc: 0.9860\n","\n","Epoch 00015: val_acc did not improve from 0.98900\n","Epoch 16/25\n","1015/1015 [==============================] - 743s 732ms/step - loss: 0.0447 - acc: 0.9840 - val_loss: 0.0350 - val_acc: 0.9855\n","\n","Epoch 00016: val_acc did not improve from 0.98900\n","Epoch 17/25\n","1015/1015 [==============================] - 746s 735ms/step - loss: 0.0404 - acc: 0.9858 - val_loss: 0.0325 - val_acc: 0.9865\n","\n","Epoch 00017: val_acc did not improve from 0.98900\n","Epoch 18/25\n","1015/1015 [==============================] - 752s 741ms/step - loss: 0.0370 - acc: 0.9867 - val_loss: 0.0315 - val_acc: 0.9890\n","\n","Epoch 00018: val_acc did not improve from 0.98900\n","Epoch 19/25\n","1015/1015 [==============================] - 746s 735ms/step - loss: 0.0357 - acc: 0.9874 - val_loss: 0.0323 - val_acc: 0.9895\n","\n","Epoch 00019: val_acc improved from 0.98900 to 0.98950, saving model to weights-improvement-19-0.99.hdf5\n","Epoch 20/25\n","1015/1015 [==============================] - 745s 734ms/step - loss: 0.0338 - acc: 0.9883 - val_loss: 0.0254 - val_acc: 0.9905\n","\n","Epoch 00020: val_acc improved from 0.98950 to 0.99050, saving model to weights-improvement-20-0.99.hdf5\n","Epoch 21/25\n"," 127/1015 [==>...........................] - ETA: 10:36 - loss: 0.0333 - acc: 0.9880"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xxToqIhNT6tB","colab_type":"text"},"source":["**Converting Video into frames** "]},{"cell_type":"code","metadata":{"id":"EczzDrt6d9Iu","colab_type":"code","colab":{}},"source":["\n","import random,os\n","import cv2\n","import shutil\n","import cv2     # for capturing videos\n","import math   # for mathematical operations\n","import matplotlib.pyplot as plt    # for plotting the images\n","import pandas as pd\n","from keras.preprocessing import image   # for preprocessing the images\n","import numpy as np    # for mathematical operations\n","from keras.utils import np_utils\n","from skimage.transform import resize   # for resizing images\n","count = 0\n","for i in range(1,100):\n","    videoFile = r\"/content/action1/KTHdataset/boxing/1i_%dv.avi\"%i\n","    cap = cv2.VideoCapture(videoFile)   # capturing the video from the given path\n","    \n","    x=1\n","    while True:\n","        frameId = cap.get(1) #current frame number\n","        ret, frame = cap.read()\n","        if (ret == True):\n","            filename =r\"/content/action1/train/boxing/boxing_%d.jpg\" % count;count+=1\n","            cv2.imwrite(filename, frame)\n","        else:\n","            break\n","    cap.release()\n","    print (\"Done!\")\n","\n","\n","\n","\n","\n","    \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjDjXbGgUf3t","colab_type":"text"},"source":["**Detecting Human In The Frames**"]},{"cell_type":"code","metadata":{"id":"GGKykTgR_FNQ","colab_type":"code","colab":{}},"source":["import cv2\n","import os\n","import shutil\n","\n","video_src = 'pedestrians.avi'\n","count=0\n","for r in range(0,65000):\n","    e=r'/content/action/walking/walking_%d.jpg'%r\n","    f=r'/content/action/none1/walking_%d.jpg'%r\n","    img=cv2.imread(e)\n","\n","    bike_cascade = cv2.CascadeClassifier('/content/project1/pedx_mcs.xml')\n","    bike_cascade1 = cv2.CascadeClassifier('/content/project1/pedxlower.xml')\n","\n","        \n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    bike = bike_cascade.detectMultiScale(img,1.009,2)\n","    bike1 = bike_cascade1.detectMultiScale(img,1.001,2)\n","    if bike==():\n","        pass\n","    elif bike.all()>0:\n","        count=count+1\n","    if bike1==():\n","        pass\n","    elif bike1.all()>0:\n","        count=count+1\n","    if bike==() and bike1==():\n","        shutil.move(e,f)\n","    print(count)\n","\n","    \n","\n","cv2.destroyAllWindows()"],"execution_count":0,"outputs":[]}]}